{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f634c6-1eec-40a5-ac58-a64ba63669cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow import config as tfconfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "gpus = tfconfig.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tfconfig.set_logical_device_configuration(gpu, [tfconfig.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "    except RuntimeError as e:\n",
    "        \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283d84b5-6aa9-46ba-a147-9d01ca78ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24228 files belonging to 2 classes.\n",
      "Using 21806 files for training.\n",
      "Found 24228 files belonging to 2 classes.\n",
      "Using 2422 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory('train',\n",
    "                                             subset='training',\n",
    "                                             seed=42,\n",
    "                                             validation_split=0.1,\n",
    "                                             batch_size=32,\n",
    "                                             image_size=(128, 128))\n",
    "\n",
    "validation_dataset = image_dataset_from_directory('train',\n",
    "                                             subset='validation',\n",
    "                                             seed=42,\n",
    "                                             validation_split=0.1,\n",
    "                                             batch_size=32,\n",
    "                                             image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e10f1f-d033-47f3-a1af-f09895f9b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04dad38c-0c8d-4f19-b5a4-d37e24173b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Normalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44dcac4-850c-48a4-a920-482df8c2d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664b959-ecd1-40ca-ad5e-48bbb35998b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "682/682 [==============================] - ETA: 0s - loss: 1.1282 - accuracy: 0.8279\n",
      "Epoch 1: val_accuracy improved from -inf to 0.97853, saving model to best_model1.h5\n",
      "682/682 [==============================] - 62s 77ms/step - loss: 1.1282 - accuracy: 0.8279 - val_loss: 0.0747 - val_accuracy: 0.9785\n",
      "Epoch 2/15\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9742\n",
      "Epoch 2: val_accuracy improved from 0.97853 to 0.99174, saving model to best_model1.h5\n",
      "682/682 [==============================] - 49s 72ms/step - loss: 0.0746 - accuracy: 0.9742 - val_loss: 0.0304 - val_accuracy: 0.9917\n",
      "Epoch 3/15\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9834\n",
      "Epoch 3: val_accuracy did not improve from 0.99174\n",
      "682/682 [==============================] - 48s 71ms/step - loss: 0.0558 - accuracy: 0.9834 - val_loss: 0.0417 - val_accuracy: 0.9897\n",
      "Epoch 4/15\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.0463 - accuracy: 0.9861\n",
      "Epoch 4: val_accuracy did not improve from 0.99174\n",
      "682/682 [==============================] - 48s 71ms/step - loss: 0.0463 - accuracy: 0.9862 - val_loss: 0.0429 - val_accuracy: 0.9884\n",
      "Epoch 5/15\n",
      "179/682 [======>.......................] - ETA: 34s - loss: 0.0342 - accuracy: 0.9894"
     ]
    }
   ],
   "source": [
    "model_save_path = 'best_model1.h5'\n",
    "checkpoint_callback = ModelCheckpoint(model_save_path, \n",
    "                                      monitor='val_accuracy',\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=validation_dataset,\n",
    "                    epochs=15,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc74f51-54ff-4eb7-8055-ebd6269d4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11514 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = image_dataset_from_directory('test',\n",
    "                                             label_mode='int',\n",
    "                                             batch_size=64,\n",
    "                                             image_size=(250, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2289196-3ea0-4f2e-be70-6805886a55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'best_model1.h5'\n",
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9853dadc-b83b-41a3-b4ed-03d4e51e7738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 8s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69562e20-aa8b-44ca-8533-3c0c31358b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.rint(y_test)\n",
    "ans = ans.astype(int)\n",
    "ans = pd.DataFrame(ans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9702140-b033-4b5a-9132-fe53dd3fda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"test/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd528fce-068e-4cfc-ac67-fc1da4b8d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([image.img_to_array(image.load_img(\"test/test/\"+fname, target_size=(128, 128))) for fname in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2832fca3-982c-4d3a-bb4f-b3e43854329f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ans \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n",
      "File \u001b[1;32mc:\\users\\ximal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\ximal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "ans = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc8e9d-da9c-426d-8299-ac44a9ca2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.rint(ans)\n",
    "ans = ans.astype(int)\n",
    "ans = pd.DataFrame(ans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db2f2d-c251-4878-98bd-c8a4c15871b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.DataFrame(filenames)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdf59b-caa0-4469-a5bd-f07a20d7a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([names, ans], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff5185-099b-4752-9496-4575d721b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_axis(['filename','label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32afb47-46a0-4701-bd22-ccb2a728fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ans.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa4a3d-0d1c-4786-b8e2-4ff28fc1776b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
